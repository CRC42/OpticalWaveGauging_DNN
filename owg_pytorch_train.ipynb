{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6IDRkUomgeK"
      },
      "source": [
        "原始代码： https://colab.research.google.com/drive/1Uyz2Wsm0pUQWuZ0t6DnBUm4mnmqUERLh#scrollTo=ISwXERZACOsz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gAsJJKDflnrj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms, models\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ-XO3ZvtF8K"
      },
      "source": [
        "## 数据集："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnxpMBPGmy-Q",
        "outputId": "4e78f4b3-4d56-4b47-f26e-2990f19c2c6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting config_train.json\n"
          ]
        }
      ],
      "source": [
        "%%writefile config_train.json\n",
        "{\n",
        "  \"samplewise_std_normalization\" : true,\n",
        "  \"samplewise_center\"  : true,\n",
        "  \"input_image_format\" : \"jpg\",\n",
        "  \"input_csv_file\"     : \"snap-training-dataset.csv\",\n",
        "  \"category\"           : \"H\",\n",
        "  \"prc_lower_withheld\": 0,\n",
        "  \"prc_upper_withheld\": 0,\n",
        "\n",
        "  \"horizontal_flip\"    : false,\n",
        "  \"vertical_flip\"      : false,\n",
        "  \"rotation_range\"     : 10,\n",
        "  \"width_shift_range\"  : 0.1,\n",
        "  \"height_shift_range\" : 0.1,\n",
        "  \"shear_range\"        : 0.05,\n",
        "  \"zoom_range\"         : 0.2,\n",
        "  \"fill_mode\"          : \"reflect\",\n",
        "\n",
        "  \"img_size\"           : 128,\n",
        "  \"num_epochs\"         : 200,\n",
        "  \"test_size\"          : 0.4,\n",
        "  \"dropout_rate\"       : 0.5,\n",
        "  \"epsilon\"            : 0.0001,\n",
        "  \"min_lr\"             : 0.0001,\n",
        "  \"factor\"             : 0.8\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "unEI_10DntdD"
      },
      "outputs": [],
      "source": [
        "with open('config_train.json') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# config variables\n",
        "imsize    = int(config[\"img_size\"])\n",
        "num_epochs = int(config[\"num_epochs\"])\n",
        "test_size = float(config[\"test_size\"])\n",
        "height_shift_range = float(config[\"height_shift_range\"])\n",
        "width_shift_range = float(config[\"width_shift_range\"])\n",
        "rotation_range = float(config[\"rotation_range\"])\n",
        "samplewise_std_normalization = config[\"samplewise_std_normalization\"]\n",
        "horizontal_flip = config[\"horizontal_flip\"]\n",
        "vertical_flip = config[\"vertical_flip\"]\n",
        "samplewise_center = config[\"samplewise_center\"]\n",
        "shear_range = float(config[\"shear_range\"])\n",
        "zoom_range = float(config[\"zoom_range\"])\n",
        "dropout_rate = float(config[\"dropout_rate\"])\n",
        "epsilon = float(config[\"epsilon\"])\n",
        "min_lr = float(config[\"min_lr\"])\n",
        "factor = float(config[\"factor\"])\n",
        "input_image_format = config[\"input_image_format\"]\n",
        "input_csv_file = config[\"input_csv_file\"]\n",
        "category = config[\"category\"]\n",
        "fill_mode = config[\"fill_mode\"]\n",
        "prc_lower_withheld = config['prc_lower_withheld']\n",
        "prc_upper_withheld = config['prc_upper_withheld']\n",
        "IMG_SIZE = (imsize, imsize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46jqmcgnn60K",
        "outputId": "0c9fe316-23a1-4170-ba4e-3fa9727f3224"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-02-19 22:53:12--  https://github.com/dbuscombe-usgs/OpticalWaveGauging_DNN/raw/master/snap-training-dataset.csv\n",
            "Connecting to 127.0.0.1:7890... connected.\n",
            "Proxy request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://github.com/OpticalWaveGauging/OpticalWaveGauging_DNN/raw/master/snap-training-dataset.csv [following]\n",
            "--2024-02-19 22:53:13--  https://github.com/OpticalWaveGauging/OpticalWaveGauging_DNN/raw/master/snap-training-dataset.csv\n",
            "Reusing existing connection to github.com:443.\n",
            "Proxy request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/OpticalWaveGauging/OpticalWaveGauging_DNN/master/snap-training-dataset.csv [following]\n",
            "--2024-02-19 22:53:14--  https://raw.githubusercontent.com/OpticalWaveGauging/OpticalWaveGauging_DNN/master/snap-training-dataset.csv\n",
            "Connecting to 127.0.0.1:7890... connected.\n",
            "Proxy request sent, awaiting response... 200 OK\n",
            "Length: 43795 (43K) [text/plain]\n",
            "Saving to: ‘snap-training-dataset.csv.1’\n",
            "\n",
            "snap-training-datas 100%[===================>]  42.77K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2024-02-19 22:53:14 (476 KB/s) - ‘snap-training-dataset.csv.1’ saved [43795/43795]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/dbuscombe-usgs/OpticalWaveGauging_DNN/raw/master/snap-training-dataset.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Kbzl8KsvsQuA"
      },
      "outputs": [],
      "source": [
        "def get_and_tidy_df(base_dir, input_csv_file, image_dir, category):\n",
        "\t'''复制原数据集，使容量为原来的5倍'''\n",
        "\tdf = pd.read_csv(os.path.join(base_dir, input_csv_file))\n",
        "\tdf['path'] = df['id'].map(lambda x: os.path.join(base_dir,\n",
        "\t\t                                                image_dir,'{}'.format(x)))\n",
        "\n",
        "\tdf = df.rename(index=str, columns={\" H\": \"H\", \" T\": \"T\"})\n",
        "\n",
        "\tif category == 'H':\n",
        "\t\tmean = df['H'].mean()\n",
        "\t\tdiv = df['H'].std()\n",
        "\t\tdf['zscore'] = df['H'].map(lambda x: (x-mean)/div)\n",
        "\telif category == 'T':\n",
        "\t\tmean = df['T'].mean()\n",
        "\t\tdiv = df['T'].std()\n",
        "\t\tdf['zscore'] = df['T'].map(lambda x: (x-mean)/div)\n",
        "\telse:\n",
        "\t\tprint(\"Unknown category: \"+str(category))\n",
        "\t\tprint(\"Fix config file, exiting now ...\")\n",
        "\t\tsys.exit()\n",
        "\n",
        "\tdf.dropna(inplace = True)\n",
        "\ttry:\n",
        "\t\tdf = df.sort_values(by='time', axis=0)\n",
        "\texcept:\n",
        "\t\tdf = df.sort_values(by='id', axis=0)\n",
        "\n",
        "\tif category == 'H':\n",
        "\t\tdf['category'] = pd.cut(df['H'], 10)\n",
        "\telse:\n",
        "\t\tdf['category'] = pd.cut(df['T'], 8)\n",
        "\n",
        "\tdf['index1'] = df.index; new_df = df.groupby(['category']).apply(lambda x: x.sample(int(len(df)/2), replace = True)).reset_index(drop = True)\n",
        "\n",
        "\treturn new_df, df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Jruzi_pwtI0s"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_145503/3895373730.py:33: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  df['index1'] = df.index; new_df = df.groupby(['category']).apply(lambda x: x.sample(int(len(df)/2), replace = True)).reset_index(drop = True)\n",
            "/tmp/ipykernel_145503/3895373730.py:33: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df['index1'] = df.index; new_df = df.groupby(['category']).apply(lambda x: x.sample(int(len(df)/2), replace = True)).reset_index(drop = True)\n"
          ]
        }
      ],
      "source": [
        "# call the utils.py function get_and_tidy_df\n",
        "image_dir = 'snap_images/data'\n",
        "new_df, df = get_and_tidy_df(os.getcwd(), input_csv_file, image_dir, category)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bANGpCrZtmM1",
        "outputId": "c37b3d9a-f390-4a3a-cf13-da4df612275c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New Data Size: 4900 Old Size: 980\n",
            "train 2940 validation 1960\n"
          ]
        }
      ],
      "source": [
        "print('New Data Size:', new_df.shape[0], 'Old Size:', df.shape[0])\n",
        "\n",
        "train_df, valid_df = train_test_split(new_df,\n",
        "                    test_size = test_size,\n",
        "                    random_state = 2018,\n",
        "                    stratify = new_df['category'])\n",
        "print('train', train_df.shape[0], 'validation', valid_df.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "U67FHrDxi9m3"
      },
      "outputs": [],
      "source": [
        "if category==\"H\":\n",
        "  idx_label = 2\n",
        "elif category==\"T\":\n",
        "  idx_label = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yZDCG5JulvOE"
      },
      "outputs": [],
      "source": [
        "class WaveHeightDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.dataframe.iloc[idx, 4]\n",
        "        image = Image.open(img_path).convert('L')  # Convert to grayscale\n",
        "        label = self.dataframe.iloc[idx, idx_label]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mtzgJxxVlwLp"
      },
      "outputs": [],
      "source": [
        "# 数据转换与增强\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(IMG_SIZE),  # 调整图像大小\n",
        "    # transforms.RandomHorizontalFlip(),  # 水平翻转\n",
        "    # transforms.RandomVerticalFlip(),  # 垂直翻转\n",
        "    transforms.RandomRotation(10),  # 随机旋转\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), shear=shear_range, scale=(0.8, 1.2)),  # 位移、剪切和缩放\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # 归一化\n",
        "])\n",
        "\n",
        "# 数据加载\n",
        "train_dataset = WaveHeightDataset(train_df, transform=transform)\n",
        "test_dataset = WaveHeightDataset(valid_df, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGDGzLbJhke9",
        "outputId": "3916843b-3781-4eeb-8bfd-1d85067b3ec3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "id                                     1516464000.cx.snap.jpg\n",
              "time                                               1516464000\n",
              "H                                                        1.79\n",
              "T                                                       14.15\n",
              "path        /home/ubuntu/Documents/OpticalWaveGauging_DNN/...\n",
              "zscore                                                1.91159\n",
              "category                                       (1.692, 1.909]\n",
              "index1                                                    422\n",
              "Name: 3047, dtype: object"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.iloc[0, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KoDqBvdflx4L"
      },
      "outputs": [],
      "source": [
        "class OWGNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OWGNet, self).__init__()\n",
        "        self.mobilenet = models.mobilenet_v2(pretrained=False)\n",
        "        self.mobilenet.features[0][0] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(1280, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mobilenet.features(x)\n",
        "        x = nn.functional.adaptive_avg_pool2d(x, 1)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KExl7Xpylz_1",
        "outputId": "ca24f80d-39f7-4fe5-e758-567c941cff77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/anaconda3/envs/torch220/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/anaconda3/envs/torch220/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"device: \" + str(device))\n",
        "\n",
        "model = OWGNet().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device).float().view(-1, 1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    return running_loss / len(train_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 指定保存模型的路径\n",
        "checkpoint_path = 'checkPoints/model_checkpoint.pth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "id": "xZvFfob1l1qt",
        "outputId": "0385943b-dd46-4b76-f8cb-1f5d5552c9a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 0.21533022869540297\n",
            "Epoch 2, Train Loss: 0.15933393508843754\n",
            "Epoch 3, Train Loss: 0.1369465592560237\n",
            "Epoch 4, Train Loss: 0.116467413948039\n",
            "Epoch 5, Train Loss: 0.10425567226079495\n",
            "Epoch 6, Train Loss: 0.09378270622666764\n",
            "Epoch 7, Train Loss: 0.09128506657788935\n",
            "Epoch 8, Train Loss: 0.08420567387593506\n",
            "Epoch 9, Train Loss: 0.08307803351351101\n",
            "Epoch 10, Train Loss: 0.07039195521856131\n",
            "Epoch 11, Train Loss: 0.07051782941182508\n",
            "Epoch 12, Train Loss: 0.06459084591504348\n",
            "Epoch 13, Train Loss: 0.06202148741273129\n",
            "Epoch 14, Train Loss: 0.059925824616347316\n",
            "Epoch 15, Train Loss: 0.05940231788174614\n",
            "Epoch 16, Train Loss: 0.05287109287050755\n",
            "Epoch 17, Train Loss: 0.04995584431225839\n",
            "Epoch 18, Train Loss: 0.0432764986054162\n",
            "Epoch 19, Train Loss: 0.05160599218114563\n",
            "Epoch 20, Train Loss: 0.04867023049408327\n",
            "Epoch 21, Train Loss: 0.04418586724458019\n",
            "Epoch 22, Train Loss: 0.04557632242390157\n",
            "Epoch 23, Train Loss: 0.043489348381231335\n",
            "Epoch 24, Train Loss: 0.04443321648843424\n",
            "Epoch 25, Train Loss: 0.040758974141562765\n",
            "Epoch 26, Train Loss: 0.0396322983863723\n",
            "Epoch 27, Train Loss: 0.04155147987741815\n",
            "Epoch 28, Train Loss: 0.03902140432088028\n",
            "Epoch 29, Train Loss: 0.03685446799752991\n",
            "Epoch 30, Train Loss: 0.043583158226481275\n",
            "Epoch 31, Train Loss: 0.03436264755082843\n",
            "Epoch 32, Train Loss: 0.033854574526903096\n",
            "Epoch 33, Train Loss: 0.03347322076012421\n",
            "Epoch 34, Train Loss: 0.03067288714278813\n",
            "Epoch 35, Train Loss: 0.03641090157401303\n",
            "Epoch 36, Train Loss: 0.03668836728953149\n",
            "Epoch 37, Train Loss: 0.034391373593319695\n",
            "Epoch 38, Train Loss: 0.030399371677523723\n",
            "Epoch 39, Train Loss: 0.029890882413145966\n",
            "Epoch 40, Train Loss: 0.03208264457213733\n",
            "Epoch 41, Train Loss: 0.03948534750784545\n",
            "Epoch 42, Train Loss: 0.03424667706966157\n",
            "Epoch 43, Train Loss: 0.026765579319781744\n",
            "Epoch 44, Train Loss: 0.025984468161249937\n",
            "Epoch 45, Train Loss: 0.028124636082160894\n",
            "Epoch 46, Train Loss: 0.030680886915196545\n",
            "Epoch 47, Train Loss: 0.03579890557929226\n",
            "Epoch 48, Train Loss: 0.03803234189019903\n",
            "Epoch 49, Train Loss: 0.030137924657142517\n",
            "Epoch 50, Train Loss: 0.026030471631953413\n",
            "Epoch 51, Train Loss: 0.03079507777052324\n",
            "Epoch 52, Train Loss: 0.025294068077093234\n",
            "Epoch 53, Train Loss: 0.02313128020108232\n",
            "Epoch 54, Train Loss: 0.01913825366342359\n",
            "Epoch 55, Train Loss: 0.01839737202870943\n",
            "Epoch 56, Train Loss: 0.020489570945400097\n",
            "Epoch 57, Train Loss: 0.01671977686371816\n",
            "Epoch 58, Train Loss: 0.021056751918007176\n",
            "Epoch 59, Train Loss: 0.03004084191912704\n",
            "Epoch 60, Train Loss: 0.026298218900236585\n",
            "Epoch 61, Train Loss: 0.020380848370816395\n",
            "Epoch 62, Train Loss: 0.015280728418197807\n",
            "Epoch 63, Train Loss: 0.02099301029533229\n",
            "Epoch 64, Train Loss: 0.020345312950666994\n",
            "Epoch 65, Train Loss: 0.01797130615728826\n",
            "Epoch 66, Train Loss: 0.0175587530728713\n",
            "Epoch 67, Train Loss: 0.02141879281312551\n",
            "Epoch 68, Train Loss: 0.0183239132545524\n",
            "Epoch 69, Train Loss: 0.016646588793145897\n",
            "Epoch 70, Train Loss: 0.014447775084788547\n",
            "Epoch 71, Train Loss: 0.014349209658457372\n",
            "Epoch 72, Train Loss: 0.01780997343999131\n",
            "Epoch 73, Train Loss: 0.0251568286735362\n",
            "Epoch 74, Train Loss: 0.016670260611293918\n",
            "Epoch 75, Train Loss: 0.013897415397324316\n",
            "Epoch 76, Train Loss: 0.025292001654515447\n",
            "Epoch 77, Train Loss: 0.014995947320763345\n",
            "Epoch 78, Train Loss: 0.01440417670401866\n",
            "Epoch 79, Train Loss: 0.01164018990897128\n",
            "Epoch 80, Train Loss: 0.012213125258036282\n",
            "Epoch 81, Train Loss: 0.011516854251765282\n",
            "Epoch 82, Train Loss: 0.01751546982584683\n",
            "Epoch 83, Train Loss: 0.012135509658432768\n",
            "Epoch 84, Train Loss: 0.014245003188514838\n",
            "Epoch 85, Train Loss: 0.013319061199739657\n",
            "Epoch 86, Train Loss: 0.011908563981389465\n",
            "Epoch 87, Train Loss: 0.01191831224613175\n",
            "Epoch 88, Train Loss: 0.011654929526190719\n",
            "Epoch 89, Train Loss: 0.010432709232428233\n",
            "Epoch 90, Train Loss: 0.020063982947486573\n",
            "Epoch 91, Train Loss: 0.02406939245638964\n",
            "Epoch 92, Train Loss: 0.01567296560792981\n",
            "Epoch 93, Train Loss: 0.010681654920082783\n",
            "Epoch 94, Train Loss: 0.008848129427465408\n",
            "Epoch 95, Train Loss: 0.008435089726370516\n",
            "Epoch 96, Train Loss: 0.008294145233736819\n",
            "Epoch 97, Train Loss: 0.010280826316296083\n",
            "Epoch 98, Train Loss: 0.008015261317664028\n",
            "Epoch 99, Train Loss: 0.010759287494811755\n",
            "Epoch 100, Train Loss: 0.010546402947511524\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 100  # 根据需要调整\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    print(f'Epoch {epoch+1}, Train Loss: {train_loss}')\n",
        "    \n",
        "    # 在每个epoch后保存模型checkpoint\n",
        "    # 你可以选择保存更多的信息，如优化器的状态\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': train_loss,\n",
        "    }, checkpoint_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 加载模型checkpoint\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "loss = checkpoint['loss']\n",
        "\n",
        "# 在恢复训练之前，确保模型处于训练模式\n",
        "model.train()\n",
        "# 接着可以继续训练或进行评估..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtf85-7ll3qx"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()  # 设置模型为评估模式\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():  # 在评估阶段不计算梯度\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device).float().view(-1, 1)\n",
        "            outputs = model(images)\n",
        "            predictions.extend(outputs.view(-1).cpu().numpy())  # 将预测保存起来\n",
        "            actuals.extend(labels.view(-1).cpu().numpy())  # 保存真实标签\n",
        "    return actuals, predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAxwPCnzl50L"
      },
      "outputs": [],
      "source": [
        "actuals, predictions = evaluate_model(model, test_loader, device)\n",
        "\n",
        "# 绘制实际值与预测值的对比图\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(actuals, predictions, alpha=0.5)\n",
        "plt.plot([min(actuals), max(actuals)], [min(actuals), max(actuals)], 'r')  # 绘制理想线\n",
        "plt.xlabel('Actual Wave Height (m)')\n",
        "plt.ylabel('Predicted Wave Height (m)')\n",
        "plt.title('Actual vs. Predicted Wave Heights')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FY2LVyzFl7kH"
      },
      "outputs": [],
      "source": [
        "def show_predictions(test_loader, model, device, num_images=5):\n",
        "    model.eval()\n",
        "    images, labels = next(iter(test_loader))\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(20, 4))\n",
        "    with torch.no_grad():\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        for i in range(num_images):\n",
        "            ax = axes[i]\n",
        "            img = images[i].cpu().squeeze()  # 移除批处理维度\n",
        "            label = labels[i].item()\n",
        "            prediction = outputs[i].item()\n",
        "            ax.imshow(img.numpy(), cmap='gray')\n",
        "            ax.set_title(f'True: {label:.3f}\\nPred: {prediction:.3f}')\n",
        "            ax.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# 显示一些预测结果\n",
        "show_predictions(test_loader, model, device, num_images=5)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
